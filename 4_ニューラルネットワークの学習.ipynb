{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0975\n",
      "0.5975\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#損失関数\n",
    "def mean_squared_error(y, t):\n",
    "    return 0.5 * np.sum((y-t)**2)\n",
    "\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "print(mean_squared_error(np.array(y),np.array(t)))\n",
    "\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "print(mean_squared_error(np.array(y),np.array(t)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.510825457099\n",
      "2.30258409299\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#交差エントロピー誤差\n",
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7 #微小な数\n",
    "    return -np.sum(t * np.log(y + delta))\n",
    "\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "print(cross_entropy_error(np.array(y),np.array(t)))\n",
    "\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "print(cross_entropy_error(np.array(y),np.array(t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 10)\n",
      "[39581 27660 52431 15455  8120 39093 25838 28261 23928 26555]\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(t_train.shape)\n",
    "\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 10\n",
    "#ランダムに10個取り出す\n",
    "batch_mask = np.random.choice(train_size, batch_size)\n",
    "x_batch = x_train[batch_mask]\n",
    "t_batch = t_train[batch_mask]\n",
    "print(batch_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEPCAYAAABBUX+lAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAG9hJREFUeJzt3Xnc1XPex/HXpw1ZmjGWhrLeZiwjBiE1txOmEhNZYgZT\nlm5b6rYnN7oNjWE0mLjtSahEUVqmzZGStYWSyIgyXamRicq0XJ/7j++F5Lrqus51fud7lvfz8TgP\nV1fnOr/P4zi9+/T9fRdzd0REpPjViV2AiIjkhgJfRKREKPBFREqEAl9EpEQo8EVESoQCX0SkRCQe\n+GbWyMyGmtlcM5tjZkckfU0REfmhejm4xt3AaHc/3czqAQ1zcE0REdmIJbnwysy2A2a4+96JXURE\nRKol6SGdPYFlZtbfzKab2YNmtlXC1xQRkUokHfj1gEOAe939EGAV0DPha4qISCWSHsNfBCx09zcr\nfv0McO3GTzIzbegjIlJD7m41eX6iHb67LwEWmtnPKr51LPBuFc/VIwuPm266KXoNxfTQ+6n3M58e\nH33kNGnizJ6dWY+ci1k63YEnzaw+8Hfg3BxcU0SkqCxdCm3bwjXXwAEHZPYaiQe+u88Cmid9HRGR\nYvXll9C+PXTqBJddlvnraKVtkUmlUrFLKCp6P7NL72fN/fvfcMopcMghcPPNtXutROfhV7sIM8+H\nOkRE8kl5Ofzud7BmDQwdCnXrfvd7ZobX8KZtLsbwRUSkhtyhRw8oK4OxY78f9plS4IuI5KFbb4WX\nX4aXXoItt8zOayrwRUTyzIMPQv/+MHUqNGqUvddV4IuI5JEhQ8LN2ZdegsaNs/vaCnwRkTwxdix0\n7w4TJsDeCWw5qcAXEckDU6bA738Pzz8PBx6YzDU0D19EJLKZM+HUU+HJJ6FFi+Suo8AXEYno/ffD\nKtr77oNf/zrZaynwRUQiWbgQ2rSBW24JHX7SFPgiIhEsXRrC/rLL4LzzcnNNba0gIpJjK1bAMcdA\nu3ahu89EJlsrKPBFRHJo9Wo4/viwxXG/fmA1iuzvKPBFRPLY2rVh58vttoOBA6FOLQbVMwl8jeGL\niORAeTl06RI2RXvssdqFfaa08EpEJGHuYQXtokVhNW39+nHqUOCLiCTs+uvhlVfgxRdhq63i1aHA\nFxFJ0K23wogRkE5nd+fLTCjwRUQS8pe/wIABYefLHXaIXY0CX0QkEfffD/fcA5Mnw09/GruaQIEv\nIpJljz8ehnLSaWjaNHY131Hgi4hk0dCh0LMnTJqUzJ72taHAFxHJkpEjw94448bBvvvGruaHFPgi\nIlkwfjycfz6MGgXNmsWupnJaaSsiUkuTJ8NZZ8GwYdC8eexqqqbAFxGphddfh9NOg0GDoFWr2NVs\nmgJfRCRDM2fCb34D/fvDscfGrmbzEh/DN7MFwL+AcmCtux+e9DVFRJI2d27Y5vjee+GEE2JXUz25\nuGlbDqTcfXkOriUikrgPPgjnz95+exjOKRS5GNKxHF1HRCRx8+eH06puvhnOOSd2NTWTiyB2YLyZ\nvWFmXXNwPRGRRHz4YQj7G2/M3Tm02ZSLIZ2W7r7YzHYkBP9cd5+Sg+uKiGTNRx+FG7PXXw9dC7R1\nTTzw3X1xxX+Xmtlw4HDgB4Hfu3fvb79OpVKkUqmkSxMRqZYFC0Jnf801cOGFcWpIp9Ok0+lavUai\nZ9qaWUOgjrt/ZWZbA+OA/3X3cRs9T2faikhe+vhjaN0arrgCunWLXc13MjnTNukOf2dguJl5xbWe\n3DjsRUTy1cKFobPv0SO/wj5TiXb41S5CHb6I5JlFiyCVgksuCd19vsmkw9d0SRGRjXz6aRjGufDC\n/Az7TCnwRUQ2sHhxGMa54AK4+urY1WSXAl9EpEJZWejsO3eGa6+NXU32KfBFRIAlS0Jnf9ZZ0KtX\n7GqSocAXkZJXVhbCvlMnuOGG2NUkR4EvIiXtH/8Is3HOOAM2WP9ZlHTEoYiUrEWLQmd/7rlw3XWx\nq0meOnwRKUmffBI6+65dSyPsQYEvIiVowQI4+mi49NLim3q5KRrSEZGS8uGHYdfLq64qju0SakId\nvoiUjA8+CPPse/YsvbAHdfgiUiLmzYPjjoObbgqraEuRAl9Eit6774YzaPv0CatoS5UCX0SK2uzZ\n0KZNOHD87LNjVxOXxvBFpGhNnx46+759FfagDl9EitQrr0DHjvDAA3DyybGryQ8KfBEpOpMmwZln\nwsCB0LZt7Gryh4Z0RKSojBoVwn7oUIX9xhT4IlI0nnkGzjsPRo4MK2nl+xT4IlIUHn8cuneHcePg\niCNiV5OfNIYvIgXv/vvh1lth4kTYb7/Y1eQvBb6IFLS+faFfP3jpJdhrr9jV5DcFvogUJHf4wx/g\nySdD2DdtGrui/KfAF5GC4x42QBs9OoR948axKyoMCnwRKSjr18PFF8OsWZBOw09+EruiwqHAF5GC\n8e9/wznnwOefw4QJsO22sSsqLJqWKSIFYeVK6NAB1q0Li6sU9jWnwBeRvLd8edgEbddd4emnYYst\nYldUmBT4IpLXFi8Oq2aPOgoeeQTqaSA6YzkJfDOrY2bTzWxELq4nIsXh73+HX/0q7I1zxx1gFrui\nwparDr8H8G6OriUiRWD2bPjP/4Qrr4RevRT22ZB44JtZE6A98HDS1xKR4vDqq+H82T//OUzBlOzI\nRYf/F+BqwHNwLREpcOPHh9k4/fuHoRzJnkRvf5jZCcASd59pZimgyn+U9e7d+9uvU6kUqVQqydJE\nJA8NHgw9esCwYdCqVexq8ks6nSadTtfqNcw9ucbbzPoAZwPrgK2AbYFh7v77jZ7nSdYhIvnvrrvg\nzjthzBj4xS9iV5P/zAx3r9GdjUQD/3sXMjsauNLdO1Tyewp8kRL1zb44I0bA3/4Gu+0Wu6LCkEng\na0ariESzdi1ccAG8/z5MmaJ9cZKWsw5/k0WowxcpOStXwumnQ926MGQINGwYu6LCkkmHr5W2IpJz\ny5bBMcfAT38Kw4cr7HNFgS8iObVgAbRsGfbGefhhbZWQSwp8EcmZWbPCdMvLLoNbbtHq2VzT360i\nkhPpNHTqFM6f7dQpdjWlSR2+iCRu0KAQ8oMHK+xjUocvIolxh9tvh3vvhYkT4cADY1dU2hT4IpKI\ndeuge3eYOhWmTQuHl0hcCnwRybqVK8PGZ2vWwMsvw3bbxa5IQGP4IpJlZWXhhKodd4QXXlDY5xMF\nvohkzdy54SjCDh3CcYT168euSDakIR0RyYrJk8NWCbffDp07x65GKqPAF5FaGzw43KB96qlwUpXk\nJwW+iGTMPRwu3q8fTJgAzZrFrkg2RYEvIhlZuxYuuQTeeANeeQWaNIldkWyOAl9Eamz5cjjttLDL\n5ZQpsM02sSuS6tAsHRGpkfnzoUULOOggeO45hX0hUeCLSLW9/HLY7fK//xv69g2Hl0jh0JCOiFTL\nwIFw5ZXwxBPQpk3saiQTCnwR2aTycrjxxjDlMp2G/fePXZFkSoEvIlVavRq6dIGFC+HVV2GnnWJX\nJLWhMXwRqdSSJdC6dRinnzRJYV8MFPgi8gOzZsERR8Dxx8OTT8KWW8auSLJBQzoi8j3DhsGFF8Jf\n/xq2OJbiocAXESBsk/CHP8DDD8PYsXDoobErkmxT4IsIK1fCueeGm7Ovvw6NG8euSJKgMXyREvfJ\nJ2ExVcOG8OKLCvtiVq0O38x2AloCuwCrgdnAm+5enmBtIpKwqVPDHvZXXQWXXw5msSuSJJm7V/2b\nZq2BnsD2wAzgM2BL4GfA3sAzwJ3uvqJWRZj5puoQkex79FHo2RMGDAizcaSwmBnuXqO/ojfX4bcH\nurr7J5VcrB5wIvBr4NkqCtoCmAw0qLjWM+7+vzUpUESya906uPpqGDUqnFK1776xK5Jc2WSHn5UL\nmDV091VmVheYCnR399c3eo46fJEc+Pxz+O1vw4ycIUPgxz+OXZFkKpMOv1o3bc1soJk12uDXe5jZ\nxOr8rLuvqvhyC0KXr2QXiWDWLGjeHH7xCxg9WmFfiqo7S2cK8JqZtTezrsA44K7q/KCZ1TGzGUAZ\nMN7d38isVBHJ1KBB4azZW26BO++EepqQXZKq9b/d3R8wsznAi8Ay4JfuXlbNny0Hfmlm2wHPmdn+\n7v7uxs/r3bv3t1+nUilSqVR1Xl5ENmHdunBjdtiwcObsQQfFrkgylU6nSafTtXqNao3hm9k5wA3A\nTUAzoC1wrrvPqtHFzG4AVrp7342+rzF8kSxbtgzOOCN084MGwfbbx65IsimxMXzgVKCVuw9y9+uA\ni4DHqlHQDt+M/ZvZVoQZPe/VpEARqbnp0+Gww+Dww8N4vcJeoBazdMysgbuv2cxzDgQGEP5iqQMM\ncfdbK3meOnyRLBk4EK64Av7v/8JB41KcMunwN7fw6n+A+9z98yp+/xigobu/UKNKf/g6CnyRWlq7\nNqyYHTMGhg+HAw6IXZEkKYmFV+8AI83sa2A6sJSw0nYf4GBgAtAng1pFJIvKysJ4/bbbhs3PfvSj\n2BVJPtrcGP5p7t4S+BswB6gLrACeAA5398vdfWnCNYrIJrz0UtjKuHVrGDFCYS9V21yHf6iZ7QKc\nBbTe6Pe2ImykJiIRuMMdd0DfvmE/nLZtY1ck+W5zgX8/MBHYC3hzg+8bYcXsXgnVJSKb8MUX4XDx\nsrIwhLPbbrErkkKwySEdd7/H3fcDHnX3vTZ47OnuCnuRCGbODFMumzYNm58p7KW6Et88rVpFaJaO\nSLU88khYOavzZiWJWToikgdWrYJu3eDVV0NXv99+sSuSQqQjDkXy3Pz50KIFrF4dxusV9pIpBb5I\nHnvmmRD2XbvCU0/BNtvErkgKmYZ0RPLQ11+H7RHGjg174TRvHrsiKQbq8EXyzLx5cOSRsHQpzJih\nsJfsUeCL5JEnnoBWreCii+Dpp6FRo83/jEh1aUhHJA+sXAndu8OUKTqoRJKjDl8ksjlzwr71a9bA\nW28p7CU5CnyRSNzDQqpUCq6+Gh5/XLNwJFka0hGJYMUKuOQSmDUr7Ha5//6xK5JSoA5fJMemTYOD\nD4att4bXXlPYS+6owxfJkfXroU8f6NcP7r8fOnaMXZGUGgW+SA58/DGcfTY0aBAOGN9119gVSSnS\nkI5IwgYPDounOnSA8eMV9hKPOnyRhHz55Xc7XI4ZE44hFIlJHb5IAl57LdyY/WYIR2Ev+UAdvkgW\nrV8Pt90G99wD990Hp54auyKR7yjwRbJk/nzo3Dl09W++GY4gFMknGtIRqSV3eOCBsMPl6afDxIkK\ne8lP6vBFamHxYjj/fPjss3D0oBZRST5Thy+SoaefDjdmmzcPq2cV9pLv1OGL1NDnn4fplm+9BSNH\nhp0uRQpBoh2+mTUxs0lmNsfM3jGz7kleTyRp48aF7Yt33DGcRqWwl0Ji7p7ci5s1Bhq7+0wz2wZ4\nCzjJ3d/b6HmeZB0itbVyJVx7LYwYAY8+CscdF7siKXVmhrtbTX4m0Q7f3cvcfWbF118BcwEtLJeC\nkk5Ds2ZhS+O331bYS+HK2Ri+me0BHAy8lqtritTGV1+Frv7558PulieeGLsikdrJSeBXDOc8A/So\n6PR/oHfv3t9+nUqlSKVSuShNpFITJ8IFF0Dr1jB7NvzoR7ErklKXTqdJp9O1eo1Ex/ABzKwe8AIw\nxt3vruI5GsOXvLBiBVxzDYweHRZTHX987IpEKpd3Y/gVHgXerSrsRfLFuHFhrH79enjnHYW9FJ+k\nZ+m0BCYD7wBe8ejl7mM3ep46fInmX/+Cq64Kgf/QQ9CmTeyKRDYvkw4/0TF8d58K1E3yGiK1MWoU\nXHwxnHBC6Oq32y52RSLJ0UpbKUllZdCjR9ir/rHH4JhjYlckkjztpSMlxR0efjiM1e+9d5hXr7CX\nUqEOX0rGvHlw4YWwahVMmBBCX6SUqMOXordmDdxyC7RsCR07hp0tFfZSitThS1GbNg26doXddw+7\nW+6+e+yKROJR4EtRWrECevWCZ5+Fu+6CTp3AajSBTaT4aEhHioo7DB4M++0HX38Nc+bAGWco7EVA\nHb4UkXnz4NJLYelSGDoUjjoqdkUi+UUdvhS8Vavg+uvDTdkTTwxj9Qp7kR9Shy8FbeRI6N4djjwy\nzKnfZZfYFYnkLwW+FKQFC8JK2ffeC/vf6FASkc3TkI4UlDVr4I9/hMMOC+fJ6gQqkepThy8FY9Qo\nuPxy+PnP4Y03YM89Y1ckUlgU+JL35s0LQf/hh3D33dqnXiRTGtKRvPXNPvUtW8Kxx+pQEpHaUuBL\n3ikvh0cfhX33heXLw+KpK6+EBg1iVyZS2DSkI3ll2rQwzbJ+/TDl8rDDYlckUjwU+JIXPv0UevaE\nF1+E226Ds87Sdggi2aYhHYnqyy/hxhvDdsVNm4Z59WefrbAXSYICX6JYtw4efDBMsfzoI5gxA/r0\ngW22iV2ZSPHSkI7klDuMHQtXXw077BDG6Q89NHZVIqVBgS85M3NmCPqFC+GOO8JGZxq6EckdDelI\n4j79FM49F9q1C0cMvvMO/OY3CnuRXFPgS2K++CJsW9ysGTRuHFbMXnJJmHIpIrmnwJesW7UKbr8d\n9tkHysrCDdk//hEaNYpdmUhp0xi+ZM3atWGF7M03Q4sWMHlyOGpQRPKDAl9qrbwchgwJ8+n32AOe\new6aN49dlYhsTIEvGXOHMWOgV6+wz83994dNzkQkPyUa+Gb2CHAisMTdmyV5LcmtyZPDDdl//hNu\nvRVOPlmzbkTyXdI3bfsDbRO+huTQlCmhi+/SBc4/P0yx7NhRYS9SCBINfHefAixP8hqSG1OnhqME\nzzkHfve7MMWySxeoWzd2ZSJSXRrDl0165RXo3Rs++CAM4XTurHn0IoVK8/ClUq++Cm3bhm7+9NND\nR3/BBQp7kUKWNx1+7969v/06lUqRSqWi1VLKpk0L8+jffTd09F266KQpkXyQTqdJp9O1eg1z9+xU\nU9UFzPYARrr7gZt4jiddh1TNHcaPD6thFyyAa6+F885T0IvkMzPD3Ws0XSLpaZlPASngJ2b2CXCT\nu/dP8ppSfeXlYZFUnz6wejVcdx2ccYaGbUSKVeIdfrWKUIefU2vXwlNPwZ/+FA4c6dULOnSAOrqj\nI1Iw8q7Dl/yyejU88gj8+c+w995wzz1hTr3m0IuUBgV+Cfj887DtwT33wBFHwODBcOSRsasSkVzT\nP+KL2Pz50K1b6Obffz/cmH3+eYW9SKlS4BcZ97D9wSmnhC2KGzWCOXPgscfgwCrnSYlIKdCQTpFY\ntw6GDYM77wwbml1+OQwcCFtvHbsyEckXCvwC9+WX4Ubs3XfDrrtCz55hxo32uBGRjSnwC9S8eXDv\nvfDEE2FTs8GDww1ZEZGqKPALyPr1MHo0/PWvMGsWdO0a/tu0aezKRKQQKPALwD//Gc6Kve8+2Hnn\nMPPm9NNhiy1iVyYihUSBn8dmzIB+/cLN2A4d4OmndVasiGROgZ9nVq4Mwf7QQ7BoEVx8cZhDv+OO\nsSsTkUKnvXTyxPTpIeSHDIGWLcP4fPv2UE9/JYtIJbSXToFZsQIGDQpBv2xZOCP27behSZPYlYlI\nMVKHn2Pu8PrrIeSffRaOOQb+67/C1ErNnReR6lKHn8f+8Y+wJfGAAWHXyq5dYe5caNw4dmUiUirU\n4Sdo1aqwWdmAAfDaa2F/m86doVUr7T0vIrWjDj8PlJeHzcsefzxMpzz88BDyw4ZBw4axqxORUqbA\nz5L588NmZd9sWNa5M8yeDbvsErsyEZFAgV8Ln3wS5swPHhzmzJ95ZrgRe/DBOkVKRPKPxvBraPFi\nGDo0zJefNy+My595Jhx9tGbZiEjuZDKGr8CvhmXLQuc+ZEjY7qBDhxDyxx0H9evHrk5ESpECP4sW\nLw4zbIYPDzNs2rULId+uHWy5ZezqRKTUKfBraf78EPDDh4c58scfDx07hi0OdHKUiOQTBX4NuYch\nmuHD4bnnwtDNSSeFkG/dGho0yHlJIiLVosCvhq+/hsmTYdSoMGRTr14I+I4d4cgjtSBKRAqDFl5V\n4eOPw0lRY8ZAOg3NmoXhmhdegAMO0BRKESkNRdnhr1kDU6eGkB89GpYuDTdb27eHNm1g++2zdikR\nkShKdkjHPRwSMmECTJwIkybBz34WAr59ezjsMA3ViEhxycvAN7N2wF1AHeARd/9TJc+pceB/+mkI\n928ederAsceGR5s2sNNO2alfRCQfZRL4ifa9ZlYH6Ae0BQ4Afmtm+2byWsuXh9k03brBvvvCQQfB\nyJHQogW8+GIYp+/fH84+u7TDPp1Oxy6hqOj9zC69n3ElPdBxOPCBu3/s7muBwcBJ1fnBRYvCaVCX\nXhpusu62GzzwAOy+e/j+Z5+FLQ4uugj22Uc3Xr+hP1DZpfczu/R+xpX0LJ1dgYUb/HoR4S+BH5gz\nB15+OWwtPGVK2Eu+Vavw6NIlbEimbQxERDKXN9MyTzoJfvWrsODphhvCTVd17SIi2ZPoTVszOxLo\n7e7tKn7dE/CNb9yaWfypQiIiBSavZumYWV1gHnAssBh4Hfitu89N7KIiIlKpRId03H29mXUDxvHd\ntEyFvYhIBHmx8EpERJIXdf2pmbUzs/fM7H0zuzZmLcXAzBaY2Swzm2Fmr8eup9CY2SNmtsTM3t7g\nez82s3FmNs/M/mZmjWLWWCiqeC9vMrNFZja94tEuZo2FxMyamNkkM5tjZu+YWfeK79fo8xkt8LO5\nKEu+VQ6k3P2X7l7p9FfZpP6Ez+OGegIT3P3nwCTgupxXVZgqey8B+rr7IRWPsbkuqoCtA65w9wOA\nFsClFXlZo89nzA4/40VZUiUj8r/aCpm7TwGWb/Ttk4ABFV8PAE7OaVEFqor3EsJnVGrI3cvcfWbF\n118Bc4Em1PDzGTMcKluUtWukWoqFA+PN7A0z6xq7mCKxk7svgfCHDijhjTuyopuZzTSzhzU8lhkz\n2wM4GHgV2Lkmn091g8WlpbsfArQn/JOvVeyCipBmOWTuPmAvdz8YKAP6Rq6n4JjZNsAzQI+KTn/j\nz+MmP58xA/9TYLcNft2k4nuSIXdfXPHfpcBwqtjGQmpkiZntDGBmjYHPItdTsNx96Qbb4j4ENI9Z\nT6Exs3qEsB/o7s9XfLtGn8+Ygf8G8B9mtruZNQDOBEZErKegmVnDir/9MbOtgTbA7LhVFSTj++PM\nI4AuFV93Bp7f+AekSt97LysC6RunoM9nTT0KvOvud2/wvRp9PqPOw6+YlnU33y3Kui1aMQXOzPYk\ndPVOWFD3pN7PmjGzp4AU8BNgCXAT8BwwFGgKfAx0cvcvYtVYKKp4L1sTxp7LgQXAhd+MP8ummVlL\nYDLwDuHPuAO9CLsXPE01P59aeCUiUiJ001ZEpEQo8EVESoQCX0SkRCjwRURKhAJfRKREKPBFREqE\nAl9EpEQo8EVESoQCX2QjZnZYxUEyDcxsazObbWb7x65LpLa00lakEmZ2M7BVxWOhu/8pckkitabA\nF6mEmdUnbPC3GjjK9QdFioCGdEQqtwOwDbAtsGXkWkSyQh2+SCXM7HlgELAnsIu7Xxa5JJFaqxe7\nAJF8Y2bnAGvcfbCZ1QGmmlnK3dORSxOpFXX4IiIlQmP4IiIlQoEvIlIiFPgiIiVCgS8iUiIU+CIi\nJUKBLyJSIhT4IiIlQoEvIlIi/h+FcaNTOya6cQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x22797591d68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1999999999990898\n",
      "0.2999999999986347\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "#数値微分\n",
    "def numerical_diff(f, x):\n",
    "    h = 1e-4\n",
    "    return (f(x+h) - f(x-h)) / (2*h)\n",
    "\n",
    "def function_1(x):\n",
    "    return 0.01*x**2 + 0.1*x\n",
    "\n",
    "x = np.arange(0.0, 20.0, 0.1)\n",
    "y = function_1(x)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "plt.plot(x,y)\n",
    "plt.show()\n",
    "\n",
    "#微分する\n",
    "print(numerical_diff(function_1, 5))\n",
    "print(numerical_diff(function_1, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.00000000000378\n",
      "7.999999999999119\n"
     ]
    }
   ],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "def function_tmp1(x0):\n",
    "    return x0*x0 + 4.0**2.0\n",
    "\n",
    "print(numerical_diff(function_tmp1, 3.0))\n",
    "\n",
    "def function_tmp2(x1):\n",
    "    return 3.0**2.0 + x1*x1\n",
    "\n",
    "print(numerical_diff(function_tmp2, 4.0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6.  8.]\n",
      "[ 0.  4.]\n",
      "[-6.  0.]\n"
     ]
    }
   ],
   "source": [
    "#偏微分\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "        \n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = tmp_val\n",
    "        \n",
    "    return grad\n",
    "\n",
    "print(numerical_gradient(function_2, np.array([3.0, 4.0])))\n",
    "print(numerical_gradient(function_2, np.array([0.0, 2.0])))\n",
    "print(numerical_gradient(function_2, np.array([-3.0, 0.0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -6.11110793e-10   8.14814391e-10]\n",
      "[-3.  4.]\n",
      "[-2.45570041  3.27426722]\n"
     ]
    }
   ],
   "source": [
    "#勾配法\n",
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
    "    x = init_x\n",
    "    \n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "        \n",
    "    return x\n",
    "\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "print(gradient_descent(function_2, init_x=init_x, lr=0.1, step_num=100))\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "print(gradient_descent(function_2, init_x=init_x, lr=1, step_num=100))\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "print(gradient_descent(function_2, init_x=init_x, lr=0.001, step_num=100))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.16036434  1.04285119  0.42200119]\n",
      " [-0.95640992 -0.12211939 -0.76093539]]\n",
      "[-0.16455032  0.51580326 -0.43164114]\n",
      "1\n",
      "1.58622437408\n",
      "[[ 0.16042012  0.31676168 -0.47718179]\n",
      " [ 0.24063017  0.47514251 -0.71577269]]\n"
     ]
    }
   ],
   "source": [
    "#ニューラルネットワークの勾配を求める\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2,3) #ガウス分布で正規化\n",
    "        \n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    \n",
    "\n",
    "net = simpleNet()\n",
    "print(net.W)\n",
    "\n",
    "x = np.array([0.6, 0.9])\n",
    "p = net.predict(x)\n",
    "print(p)\n",
    "print(np.argmax(p))\n",
    "\n",
    "t = np.array([0, 0, 1])\n",
    "print(net.loss(x, t))\n",
    "\n",
    "def f(W):\n",
    "    return net.loss(x, t)\n",
    "\n",
    "dW = numerical_gradient(f, net.W)\n",
    "print(dW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#2層ニューラルネットワーク\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class TwoLayerNet:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        #重みの初期化\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "        \n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        return y\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return cross_entropy_error(y, t)\n",
    "        \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        return grads\n",
    "    \n",
    "    def gradient(self, x, t):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        grads = {}\n",
    "        \n",
    "        batch_num = x.shape[0]\n",
    "        \n",
    "        # forward\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        # backward\n",
    "        dy = (y - t) / batch_num\n",
    "        grads['W2'] = np.dot(z1.T, dy)\n",
    "        grads['b2'] = np.sum(dy, axis=0)\n",
    "        \n",
    "        da1 = np.dot(dy, W2.T)\n",
    "        dz1 = sigmoid_grad(a1) * da1\n",
    "        grads['W1'] = np.dot(x.T, dz1)\n",
    "        grads['b1'] = np.sum(dz1, axis=0)\n",
    "\n",
    "        return grads\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc | 0.0987166666667, 0.098\n",
      "train acc, test acc | 0.772283333333, 0.7811\n",
      "train acc, test acc | 0.876533333333, 0.881\n",
      "train acc, test acc | 0.898433333333, 0.9029\n",
      "train acc, test acc | 0.9056, 0.9104\n",
      "train acc, test acc | 0.914583333333, 0.9172\n",
      "train acc, test acc | 0.919966666667, 0.9223\n",
      "train acc, test acc | 0.924183333333, 0.9271\n",
      "train acc, test acc | 0.92865, 0.9301\n",
      "train acc, test acc | 0.9319, 0.9321\n",
      "train acc, test acc | 0.93555, 0.9361\n",
      "train acc, test acc | 0.9379, 0.9369\n",
      "train acc, test acc | 0.94035, 0.9395\n",
      "train acc, test acc | 0.942883333333, 0.9416\n",
      "train acc, test acc | 0.94515, 0.9428\n",
      "train acc, test acc | 0.947116666667, 0.9459\n",
      "train acc, test acc | 0.947916666667, 0.9461\n",
      "train acc, test acc | 0.950366666667, 0.9481\n",
      "train acc, test acc | 0.951333333333, 0.9488\n",
      "train acc, test acc | 0.9527, 0.9513\n",
      "train acc, test acc | 0.953516666667, 0.9505\n",
      "train acc, test acc | 0.95495, 0.9523\n",
      "train acc, test acc | 0.956066666667, 0.9537\n",
      "train acc, test acc | 0.9567, 0.953\n",
      "train acc, test acc | 0.958133333333, 0.9551\n",
      "train acc, test acc | 0.9588, 0.9549\n",
      "train acc, test acc | 0.960166666667, 0.9553\n",
      "train acc, test acc | 0.960966666667, 0.9565\n",
      "train acc, test acc | 0.96155, 0.9575\n",
      "train acc, test acc | 0.962466666667, 0.9574\n",
      "train acc, test acc | 0.963733333333, 0.9592\n",
      "train acc, test acc | 0.96455, 0.9601\n",
      "train acc, test acc | 0.965333333333, 0.9611\n",
      "train acc, test acc | 0.965566666667, 0.9603\n",
      "train acc, test acc | 0.966316666667, 0.9612\n",
      "train acc, test acc | 0.9673, 0.9625\n",
      "train acc, test acc | 0.967966666667, 0.9615\n",
      "train acc, test acc | 0.968333333333, 0.9632\n",
      "train acc, test acc | 0.968666666667, 0.9625\n",
      "train acc, test acc | 0.969433333333, 0.9639\n",
      "train acc, test acc | 0.969966666667, 0.9634\n",
      "train acc, test acc | 0.9704, 0.9649\n",
      "train acc, test acc | 0.971366666667, 0.9638\n",
      "train acc, test acc | 0.971116666667, 0.9645\n",
      "train acc, test acc | 0.972166666667, 0.9653\n",
      "train acc, test acc | 0.972183333333, 0.9664\n",
      "train acc, test acc | 0.972816666667, 0.9659\n",
      "train acc, test acc | 0.973466666667, 0.9674\n",
      "train acc, test acc | 0.974033333333, 0.9668\n",
      "train acc, test acc | 0.9744, 0.9678\n",
      "train acc, test acc | 0.974133333333, 0.9676\n",
      "train acc, test acc | 0.974933333333, 0.967\n",
      "train acc, test acc | 0.975716666667, 0.968\n",
      "train acc, test acc | 0.97555, 0.9679\n",
      "train acc, test acc | 0.9761, 0.9696\n",
      "train acc, test acc | 0.976483333333, 0.9694\n",
      "train acc, test acc | 0.976466666667, 0.9687\n",
      "train acc, test acc | 0.976833333333, 0.9696\n",
      "train acc, test acc | 0.977016666667, 0.9701\n",
      "train acc, test acc | 0.97745, 0.9688\n",
      "train acc, test acc | 0.977533333333, 0.9704\n",
      "train acc, test acc | 0.97845, 0.9707\n",
      "train acc, test acc | 0.9783, 0.9704\n",
      "train acc, test acc | 0.978766666667, 0.9704\n",
      "train acc, test acc | 0.978933333333, 0.9698\n",
      "train acc, test acc | 0.979166666667, 0.9716\n",
      "train acc, test acc | 0.979333333333, 0.9711\n",
      "train acc, test acc | 0.979733333333, 0.971\n",
      "train acc, test acc | 0.9798, 0.9705\n",
      "train acc, test acc | 0.98015, 0.9715\n",
      "train acc, test acc | 0.980316666667, 0.9713\n",
      "train acc, test acc | 0.98055, 0.9717\n",
      "train acc, test acc | 0.98075, 0.9723\n",
      "train acc, test acc | 0.980666666667, 0.9717\n",
      "train acc, test acc | 0.9812, 0.9722\n",
      "train acc, test acc | 0.981166666667, 0.9724\n",
      "train acc, test acc | 0.9815, 0.9724\n",
      "train acc, test acc | 0.981533333333, 0.9729\n",
      "train acc, test acc | 0.981966666667, 0.9724\n",
      "train acc, test acc | 0.981933333333, 0.9725\n",
      "train acc, test acc | 0.982416666667, 0.9722\n",
      "train acc, test acc | 0.982366666667, 0.9721\n",
      "train acc, test acc | 0.982616666667, 0.9726\n",
      "train acc, test acc | 0.9829, 0.9731\n"
     ]
    }
   ],
   "source": [
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "#1エポックあたりの繰り返し数\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "#ハイパーパラメータ\n",
    "iters_num = 50000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    #grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))\n",
    "\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
